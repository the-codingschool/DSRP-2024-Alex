```{r}
library(readr)
library(dplyr)
library(data.table)
```

```{r}
data_file_path = '/Users/maxshi/Documents/GitHub/DSRP-2024-Alex/data/docword.nytimes_head.txt'
vocab_file_path = '/Users/maxshi/Documents/GitHub/DSRP-2024-Alex/data/vocab.nytimes.txt'
```

```{r}
data <- fread(data_file_path)
colnames(data) <- c("article_id", "word_id", "word_id_occurrences")
```

```{r}
vocab <- fread(vocab_file_path)
colnames(vocab) <- c("word")
vocab_vector <- setNames(1:nrow(vocab), vocab$word)
length(vocab_vector)
```



```{r}
metadata <- c(300000, 102660, 69679427)

total_articles <- as.integer(metadata[1])
vocabulary_size <- as.integer(metadata[2])
total_words <- as.integer(metadata[3])

# Step 2: Read the data using fread=
data <- fread(text = paste(data_lines, collapse = "\n"), header = FALSE, sep = " ")

# Assign column names
colnames(data) <- c("article_id", "word_id", "word_id_occurrences")
```

```{r}
small_data <- filter(data, article_id <= 1000)
small_data
```

```{r}
num =  as.integer(small_data[1, "word_id"])
vector1 = c(1, 2, 3)
vector2 = c(4, 5, 6)
sample_list = list()
sample_list[[1]] <- vector1
class(sample_list[[1]])
```


```{r}

vector_list <- list()
for (x in unique(small_data$article_id)) {
  article_vector <- numeric(length(vocab_vector)) # Initialize with zeros
  article_data <- filter(small_data, article_id == x)
  
  for (i in 1:nrow(article_data)) {
    word_index <- as.integer(article_data[i, 'word_id'])
    word_count <- as.integer(article_data[i, 'word_id_occurrences'])
    article_vector[word_index] <- word_count
  }
  
  # Add data from data table into article vector
  vector_list[[x]] <- article_vector
  if(x %% 100 == 0) {
    cat("processed ", x)
  }
}

# Print the vectors and dataframes for verification
vector1 = vector_list[[1]]
vector2 = vector_list[[2]]
filter(small_data, article_id == 1)
filter(small_data, article_id == 2)
#print(vector1)
print(vector2)
```
```{r}
as.integer(vector1 %*% vector2)
```

```{r}
# Number of articles (vectors)
n <- length(vector_list)

# Initialize an n x n matrix to store the dot products
dot_product_matrix <- matrix(0, n, n)

# Compute pairwise dot products and store them in the matrix
for (i in 1:n) {
  for (j in (i:n)) {
    if(i <= n && j <= n) {
      dot_product_matrix[i, j] <- as.integer(vector_list[[i]] %*% vector_list[[j]])
    }
  }
  cat("done processing i = ", i)
}

```

```{r}
dot_product_matrix
```


```{r}
off_diag_elements <- dot_product_matrix[upper.tri(dot_product_matrix, diag = FALSE)]
mean_value <- mean(off_diag_elements)
median_value <- median(off_diag_elements)
sd_value <- sd(off_diag_elements)
cat("Mean:", mean_value, "\n")
cat("Median:", median_value, "\n")
cat("Standard Deviation:", sd_value, "\n")
```


```{r}
# Assuming dot_product_matrix is your 1000x1000 matrix

# Step 1: Create a logical matrix for the upper triangle without the diagonal
upper_tri_indices <- upper.tri(dot_product_matrix, diag = FALSE)

# Step 2: Extract row and column indices
row_indices <- row(dot_product_matrix)[upper_tri_indices]
column_indices <- col(dot_product_matrix)[upper_tri_indices]

# Step 3: Extract the off-diagonal elements
off_diag_elements <- dot_product_matrix[upper_tri_indices]

# Step 4: Combine values and indices into a data frame
off_diag_data <- data.frame(
  row = row_indices,
  column = column_indices,
  value = off_diag_elements
)

# Step 5: Identify the row/column pair with the largest off-diagonal element
max_similarity <- off_diag_data[which.max(off_diag_data$value), ]

#Print the results
cat("Row with highest similarity:", max_similarity$row, "\n")
cat("Column with highest similarity:", max_similarity$column, "\n")
cat("Highest similarity (dot product):", max_similarity$value, "\n")

```
```{r}
filter(off_diag_data, value > 3000)
```

```{r}
library(ggplot2)
graphing_data <- data.frame(value = off_diag_elements)
ggplot(graphing_data, aes(x = value)) +
  geom_histogram(binwidth = 10, fill = "blue", color = "black", alpha = 0.7) +
  labs(title = "Histogram of Off-Diagonal Elements",
       x = "Dot Product Value",
       y = "Frequency") +
  xlim(0, 1000) +
  theme_minimal()
```
Investigate Outlier
```{r}
vector_list[[722]] %*% vector_list[[215]]
filter(small_data, article_id == 722)
filter(small_data, article_id == 215)
```
```{r}

vocab_vector[14758]

```
Conclusions: There's a bunch of repeated nonsensical words like ffff, causing similarity scores to skyrocket. The data must be cleaned to only include words which contain information. 

