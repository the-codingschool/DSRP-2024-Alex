Loading in libraries

```{r}
library(stringi)
library(htmlwidgets)
library(lsa)
library(Matrix)
library(textreuse)
library(profvis)
library(data.table) # this library contains the fread() function
library(ggplot2)
library(dplyr)
library(tidyr)
library(reshape2)
library(pheatmap) # for clustering and reordering
```

Read in the dataset and name the columns

```{r}
data <- fread("data/docword.nytimes.txt.gz")

vocab <- fread("data/vocab.nytimes.txt")
```

Adding meaningful column names to the dataset

```{r}
colnames(data) <- c("article_id", "word_id", "word_occurrences")

data
```

First convert the data table into a list of vectors for each article.

```{r}
# Create a smaller dataset containing chosen number of articles because the dataset contains too many articles
small_data <- filter(data, article_id <= 1000)

colnames(vocab) <- c("word")
vocab_vector <- setNames(1:nrow(vocab), vocab$word)

vector_list <- list()
for (x in unique(small_data$article_id)) {
  article_vector <- numeric(length(vocab_vector)) # Initialize with zeros
  article_data <- filter(small_data, article_id == x)
  
  for (i in 1:nrow(article_data)) {
    word_index <- as.integer(article_data[i, 'word_id'])
    word_count <- as.integer(article_data[i, 'word_occurrences'])
    article_vector[word_index] <- word_count
  }
  
  # Add data from data table into article vector
  vector_list[[x]] <- article_vector
  
  # Check progress by printing what article is processed for every 100 articles (only applies when small_data contains over 100 articles)
 if(x %% 100 == 0) {
    cat("processed ", x)
  }
}

# Print the vectors and dataframes for verification
vector1 = vector_list[[1]]
vector2 = vector_list[[2]]
filter(small_data, article_id == 1)
filter(small_data, article_id == 2)
print(vector1)
print(vector2)
```

**Basic algorithm for calculating similarity scores using cosine similarity**

The cosine similarity matrix consists of n x n entries of the form (i, j), representing the cosine similarity between vector i and vector j. The matrix is symmetric, and diagonal entries represent the similarity of each vector with itself (which equals 1, unless there are any zero vectors).

```{r}
basic_algorithm <- function(vector_list) {

# Number of vectors
n <- length(vector_list)

# Compute vector magnitudes
# the sapply function is used to apply a function to each vector in vector_list
vector_magnitudes <- sapply(vector_list, function(vec) sqrt(sum(vec^2)))

# Create a matrix for the cosine similarity
cosine_similarity_matrix <- matrix(0, n, n)

# Compute pairwise cosine similarity
for (i in 1:n) {
  for (j in i:n) {
    # Compute dot product
    dot_product <- as.integer(vector_list[[i]] %*% vector_list[[j]])
    
    # Get magnitudes
    magnitude_i <- vector_magnitudes[i]
    magnitude_j <- vector_magnitudes[j]
    
    # Calculate cosine similarity
    if (magnitude_i == 0 || magnitude_j == 0) {
      cosine_similarity_matrix[i, j] <- NA  # If the vector is empty
    } else {
      cosine_similarity_matrix[i, j] <- dot_product / (magnitude_i * magnitude_j)
    }
    
    # Since cosine similarity is symmetric, copy the value to complete the rest of the matrix
    cosine_similarity_matrix[j, i] <- cosine_similarity_matrix[i, j]
    }
  }
}

cosine_similarity_matrix <- basic_algorithm(vector_list)
```

Now let's use the profvis package to measure the efficiency of the basic algorithm that uses cosine similarity.

```{r}
# Profile the algorithm
basic_algorithm_prof_results <- profvis({
 basic_algorithm(vector_list)
})

basic_algorithm_prof_results
```

Save the basic cosine similarity algorithm profvis profile as an HTML file.

```{r}
saveWidget(basic_algorithm_prof_results, file = "basic_algorithm_profvis_profile.html")
```

**Locality Sensitive Hashing (LSH) algorithm: SimHash**

Now, let's use a different, more efficient, Locality Sensitive Hashing (LSH) algorithm called SimHash to perform the same task of finding similarity scores.

```{r}
names(vector_list) <- as.character(unique(small_data$article_id))

# Function to compute SimHash value
compute_simHash <- function(vector, hash_size = 64) {
  # Initialize bit vector
  bit_vector <- rep(0, hash_size)
  
  # Compute hash for each bit position
  for (i in 1:hash_size) {
    bit_sum <- sum(vector * (i %% 2 * 2 - 1))  # Simple bitwise operation
    bit_vector[i] <- ifelse(bit_sum >= 0, 1, 0)
  }
  
  # Convert bit vector to string
  simhash <- paste(bit_vector, collapse = "")
  return(simhash)
}

# Function to calculate Hamming distance
hamming_distance <- function(hash1, hash2) {
  sum(strsplit(hash1, NULL)[[1]] != strsplit(hash2, NULL)[[1]])
}

# Main function to compute similarity matrix
simHash_algorithm <- function(vector_list, hash_size = 64) {
  # Compute SimHash for each article
  simhash_list <- list()
  for (article_id in names(vector_list)) {
    article_vector <- vector_list[[article_id]]
    simhash <- compute_simHash(article_vector, hash_size)
    simhash_list[[article_id]] <- simhash
  }
  
  # Compute pairwise similarity scores (Hamming distance matrix)
  article_ids <- names(simhash_list)  # Ensure consistent ordering
  num_articles <- length(article_ids)
  simHash_similarity_matrix <- matrix(0, nrow = num_articles, ncol = num_articles)
  rownames(simHash_similarity_matrix) <- article_ids
  colnames(simHash_similarity_matrix) <- article_ids

  for (i in 1:num_articles) {
    for (j in i:num_articles) {
      if (article_ids[i] %in% names(simhash_list) && article_ids[j] %in% names(simhash_list)) {
        dist <- hamming_distance(simhash_list[[article_ids[i]]], simhash_list[[article_ids[j]]])
        simHash_similarity_matrix[i, j] <- dist
        simHash_similarity_matrix[j, i] <- dist  # Symmetric matrix
      }
    }
  }
}

simHash_similarity_matrix <- simHash_algorithm(vector_list)
```

Now let's use the profvis package to measure the efficiency of the SimHash algorithm.

```{r}
# Profile the algorithm
simHash_algorithm_prof_results <- profvis({
 simHash_algorithm(vector_list)
})

simHash_algorithm_prof_results
```

Save the SimHash algorithm profvis profile as an HTML file.

```{r}
saveWidget(simHash_algorithm_prof_results, file = "simHash_algorithm_profvis_profile.html")
```

**Accuracy**

Now, let's check the accuracy of each of the matrices. To start, we need to set a threshold similarity score to categorize article pairs as either similar (if their similarity score is above the threshold) or not similar (if their similarity score is below the threshold).

Let's start by setting the threshold for both algorithms.

```{r}
threshold <- 0.5

# Setting threshold for the basic algorithm by creating a new binary matrix
binary_similarity_matrix <- ifelse(cosine_similarity_matrix >= threshold, 1, 0)


# Setting threshold for the SimHash algorithm
```
